# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IAVDttbeSjNXO3ma6gyzxz2S3Hkul7QF
"""

!pip install -U langchain-community

!pip install -U langchain

from langchain.document_loaders import TextLoader
loader = TextLoader(file_path="/content/sample1.txt")
data = loader.load()

#chunks
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Initialize text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50,
    separators=["\n\n", "\n", " ", ""]
)

# Initialize the list to store chunks
all_chunks = []


metadata = {}

# Split documents and create metadata
for doc_idx, doc in enumerate(data):
    chunks = text_splitter.split_text(doc.page_content)
    for chunk_idx, chunk in enumerate(chunks):

        metadata[len(all_chunks)] = chunk
        all_chunks.append(chunk)

# Print chunks for verification
for i, chunk in enumerate(all_chunks):
    print(f"Chunk {i+1}:\n{chunk}\n{'-'*50}")

#embedding
from langchain.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer

embed_model = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
embeddings = embed_model.embed_documents(all_chunks)

# Print sample embeddings
print(f"Sample embedding for first chunk:\n{embeddings[0]}")

!pip install chromadb
import chromadb
from chromadb.config import Settings

# Initialize the Chroma client
client = chromadb.Client(Settings(
    persist_directory="./chroma_storage",
    anonymized_telemetry=False
))

# Define the collection name
collection_name = "document_embeddings"
# Create or retrieve the collection
collection = client.get_or_create_collection(name=collection_name)

# Create metadata for each chunk
metadata = [{"chunk": chunk, "source": file_path} for chunk in all_chunks]

# Add the chunks, embeddings, and metadata to the collection
collection.add(
    embeddings=embeddings,
    metadatas=metadata,
    ids=[str(i) for i in range(len(all_chunks))]
)

# Debugging: Confirm data was added
print(f"Chroma collection '{collection_name}' created and populated with {len(all_chunks)} items.")

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Install the latest version of langchain if not already done
!pip install -U langchain

# Import updated modules
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load GPT-2 model and tokenizer
model_name = "gpt2"  # You can use "gpt2-medium", "gpt2-large", or "gpt2-xl" for larger models
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Prepare the query and context
query_text = "What is RAG?"
# Assuming you have initialized the Chroma collection as `collection`
results = collection.query(
    query_texts=[query_text],  # The query text
    n_results=5  # Number of chunks to retrieve
)

# Debugging: Check the structure of results
print(results)
# Assuming `results` is correctly defined before this code
retrieved_chunks = [results['metadatas'][0][i]['chunk'] for i in range(len(results['documents'][0]))]

# Combine query and context into a prompt for GPT-2
context = "\n".join(retrieved_chunks)  # Combine retrieved chunks into a single context
prompt = f"Query: {query_text}\n\nContext:\n{context}\n\nAnswer:"

# Step 4.1: Set pad_token_id to eos_token_id to avoid warnings
model.config.pad_token_id = model.config.eos_token_id

# Tokenize the prompt
inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=1024, truncation=True)

# Generate a response using GPT-2
output = model.generate(
    inputs,
    max_new_tokens=100,  # Number of tokens to generate
    no_repeat_ngram_size=2,  # Avoid repeating phrases
    temperature=0.7,  # Sampling temperature (lower = more deterministic)
    top_k=50,  # Top-k sampling
    top_p=0.9,  # Top-p nucleus sampling
    do_sample=True  # Enable sampling
)

import warnings
warnings.filterwarnings("ignore", message=".*attention mask and the pad token id were not set.*")

# Decode and print the response
generated_response = tokenizer.decode(output[0], skip_special_tokens=True)
print(f"Generated Response:\n{generated_response}")